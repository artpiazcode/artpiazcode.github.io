---
layout: post
title: BERT
date: 2023-06-20
tags: transformer
categories: concepts
giscus_comments: false
related_posts: true
related_publications: true
toc:
  sidebar: left
---
* 2018

* Most cited papers 

* Largest Model : 340M parameters

* Bidirectional Encoder Representations from Transformers

* Improves upon standard transformers 

  - Removes the unidirectionality constraint by using a masked language model (MLM) pretraining objective. 

  - The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.

  - Unlike Left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. 

  - In addition to MLM, BERT also uses a next sentence prediction task that jointly pre-trains text-pair representations. 

* There are two steps in BERT: pre-training and fine-tuning. 

  - **During pre-training, the model is trained on unlabeled data over different pre-training tasks.** 

  - For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. 

  - Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters.
  
![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeN9jIi6JyotUFSk8CWwjyXPX7Teui-zJL93tGPl_jUGkDpQfPpy7HF1ELzeWXI_33przajXRaU2rFDTf-OmJAomE_xH8TmXM9yJOYyHqrznyknFTdl9N8vzcjuARIdHfEuygH1u0cndgErkX_l_tfeBZcA?key=f7C-3cR0JMOdp_HXQpeORQ)\

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdQNCiOLjNqAHu0_P-lo_aVpQBjC4f3XN8dtC7EP8iyM_MPVFyC3GroWeyBI0geqrzmniWnw-FPL713703JBfv1cZJthK4_Jx44TNNS0Nmn6Nh39zBejLUowp-cxHpw5biU7FJ7h9L-ml3-46USmG0cO-R2?key=f7C-3cR0JMOdp_HXQpeORQ)