---
layout: post
title: ModernBERT
date: 2024-12-25
description: ModernBERT
tags: bert
categories: paper-readings
giscus_comments: false
related_posts: true
related_publications: true
toc:
  sidebar: left
---

## Key takeaways
-- Latest encoder-only transformer model. Biggest leap since RoBERTa.
-- Developed by Answer.AI 
-- Modernized transformer architecture
-- Attention to efficiency
-- Increased data scales and sources


## Links
[Huggingface model](https://huggingface.co/docs/transformers/main/en/model_doc/modernbert)

## Training details

-- 2 trillion tokens
-- Native sequence length 8192 tokens

## Architectural improvements

## Rotary positional embeddings
Why are ro

### Bias terms
What are bias terms?

