<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://artpiazcode.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://artpiazcode.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-06T02:23:02+00:00</updated><id>https://artpiazcode.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">ModernBERT</title><link href="https://artpiazcode.github.io/blog/2024/ModernBERT/" rel="alternate" type="text/html" title="ModernBERT"/><published>2024-12-25T00:00:00+00:00</published><updated>2024-12-25T00:00:00+00:00</updated><id>https://artpiazcode.github.io/blog/2024/ModernBERT</id><content type="html" xml:base="https://artpiazcode.github.io/blog/2024/ModernBERT/"><![CDATA[<h2 id="key-takeaways">Key takeaways</h2> <p>– Latest encoder-only transformer model. Biggest leap since RoBERTa. – Developed by Answer.AI – Modernized transformer architecture – Attention to efficiency – Increased data scales and sources</p> <h2 id="links">Links</h2> <p><a href="https://huggingface.co/docs/transformers/main/en/model_doc/modernbert">Huggingface model</a></p> <h2 id="training-details">Training details</h2> <p>– 2 trillion tokens – Native sequence length 8192 tokens</p> <h2 id="architectural-improvements">Architectural improvements</h2> <h2 id="rotary-positional-embeddings">Rotary positional embeddings</h2> <p>Why are ro</p> <h3 id="bias-terms">Bias terms</h3> <p>What are bias terms?</p>]]></content><author><name></name></author><category term="paper-readings"/><category term="bert"/><summary type="html"><![CDATA[ModernBERT]]></summary></entry></feed>