<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://artpiazcode.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://artpiazcode.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-06T02:56:37+00:00</updated><id>https://artpiazcode.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">ModernBERT</title><link href="https://artpiazcode.github.io/blog/2024/ModernBERT/" rel="alternate" type="text/html" title="ModernBERT"/><published>2024-12-25T00:00:00+00:00</published><updated>2024-12-25T00:00:00+00:00</updated><id>https://artpiazcode.github.io/blog/2024/ModernBERT</id><content type="html" xml:base="https://artpiazcode.github.io/blog/2024/ModernBERT/"><![CDATA[<h3 id="key-takeaways">Key takeaways</h3> <ul> <li>Latest encoder-only transformer model. Biggest leap since RoBERTa.</li> <li>Developed by Answer.AI</li> <li>Modernized transformer architecture</li> <li>Attention to efficiency</li> <li>Increased data scales and sources</li> </ul> <h3 id="training-details">Training details</h3> <ul> <li>2 trillion tokens</li> <li>Native sequence length 8192 tokens</li> </ul> <h3 id="architectural-improvements">Architectural improvements</h3> <p><img src="positional_encoding.png" alt="alt text"/></p> <h4 id="rotary-positional-embeddings">Rotary positional embeddings</h4> <p>This was first introduced in RoFormer paer https://arxiv.org/pdf/2104.09864. The goal for position encoding is to enable supervision for dependency modeling between elements at different positions of the sequence. RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self attention formulation.</p> <ul> <li>Flexibility of sequence length</li> <li>Decaying inter-token dependency with increasing relative distances</li> <li>Capability if equipping the linear self-attention with relative position encoding.</li> </ul> <h4 id="bias-terms">Bias terms</h4> <p>What are bias terms?</p> <h3 id="references">References</h3> <p><a href="https://huggingface.co/docs/transformers/main/en/model_doc/modernbert">Huggingface model</a></p>]]></content><author><name></name></author><category term="paper-readings"/><category term="bert"/><summary type="html"><![CDATA[ModernBERT]]></summary></entry></feed>