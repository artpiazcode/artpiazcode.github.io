<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Transformers | Arpita Saha </title> <meta name="author" content="Arpita Saha"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://artpiazcode.github.io/blog/2023/Transformers/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Arpita</span> Saha </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/learnings/index.html">Learnings </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Past Work </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Transformers</h1> <p class="post-meta"> Created in June 01, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   ·   <a href="/blog/category/concepts"> <i class="fa-solid fa-tag fa-sm"></i> concepts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="transformers">Transformers</h1> <p><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="external nofollow noopener" target="_blank">https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a> </p> <p>Transformers (<a href="https://arxiv.org/pdf/1706.03762.pdf" rel="external nofollow noopener" target="_blank">Vaswani et al, 2017</a>) combine attention and the encoder-decoder structure to improve upon some of the limitations of RNNs in language modelling tasks like translation. Introduces a new concept - self-attention.</p> <h2 id="self-attention-layer">Self attention (layer)</h2> <p>A neural network layer that transforms a sequence of embeddings (for instance, <a href="https://developers.google.com/machine-learning/glossary#token" rel="external nofollow noopener" target="_blank">token</a> embeddings) into another sequence of embeddings. Each embedding in the output sequence is constructed by integrating information from the elements of the input sequence through an <a href="https://developers.google.com/machine-learning/glossary#attention" rel="external nofollow noopener" target="_blank">attention</a> mechanism.</p> <p>The self part of self-attention refers to the sequence attending to itself rather than to some other context. Self-attention is one of the main building blocks for <a href="https://developers.google.com/machine-learning/glossary#Transformer" rel="external nofollow noopener" target="_blank">Transformers</a> and uses dictionary lookup terminology, such as “query”, “key”, and “value”.</p> <p>A self-attention layer starts with a sequence of input representations, one for each word. The input representation for a word can be a simple embedding. For each word in an input sequence, the network scores the relevance of the word to every element in the whole sequence of words. <strong>The relevance scores determine how much the word’s final representation incorporates the representations of other words.</strong></p> <ul> <li> <p>Self attention allows a cell to compare the content of an input to all other inputs in the sequence. </p> </li> <li> <p><strong>Includes the relationship between them into the embedding.</strong></p> </li> <li> <p>For words: self-attention allows representing which other words in a sentence it has strong relationships with.</p> </li> </ul> <p>For example, consider the following sentence:</p> <p>The animal didn’t cross the street because it was too tired.</p> <p>The following illustration (from <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="external nofollow noopener" target="_blank">Transformer: A Novel Neural Network Architecture for Language Understanding</a>) shows a self-attention layer’s attention pattern for the pronoun it, with the darkness of each line indicating how much each word contributes to the representation:</p> <p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcFOSGoyk7hi3ITjd5LiLGN86qc1H2xfyZTvEow219wdpXK6l9lSuk6w7meHCFWZwQf8Blxst4qjx-Ggdtb7qilb5C2Svkf--dSl5w6jF3LDd61gMUZc4gueN0Ckgno3PNzGkLqEX6lylEay-meh3_ZJ7_C?key=f7C-3cR0JMOdp_HXQpeORQ" alt="The following sentence appears twice: 'The animal didn't cross the street because it was too tired.' Lines connect the word 'it' in one sentence to five tokens ('The', 'animal', 'street', 'it', and the period) in the other sentence. The line between 'it' and 'animal' is strongest."></p> <p>The self-attention layer highlights words that are relevant to “it”. In this case, the attention layer has learned to highlight words that it might refer to, assigning the highest weight to animal.</p> <p>For example, deciding on the most likely meaning and appropriate representation of the word “bank” in the sentence “I arrived at the bank after crossing the…” requires knowing if the sentence ends in “… road.” or “… river.”</p> <p>To compute the next representation for a given word - “bank” for example - <strong>the Transformer compares it to every other word in the sentence.</strong> The <strong>result of these comparisons is an attention score for every other word in the sentence.</strong> These attention scores determine how much each of the other words should contribute to the next representation of “bank”. In the example, the disambiguating “river” could receive a high attention score when computing a new representation for “bank”. <strong>The attention scores are then used as weights for a weighted average of all words’ representations which is fed into a fully-connected network to generate a new representation for “bank”, reflecting that the sentence is talking about a river bank.</strong></p> <p><strong>For a sequence of n</strong> <a href="https://developers.google.com/machine-learning/glossary#token" rel="external nofollow noopener" target="_blank"><strong>tokens</strong></a><strong>, self-attention transforms a sequence of embeddings n separate times, once at each position in the sequence.</strong></p> <p><strong>Attention vs self-attention:</strong></p> <ul> <li> <p>Attention allows output to focus attention on input while producing output.</p> </li> <li> <p>Self-attention allows inputs to interact with each other i.e. calculate attention of all other inputs wrt one input.</p> </li> </ul> <h2 id="multi-head-self-attention"> <strong>Multi-head self-attention</strong>.</h2> <p>An extension of <a href="https://developers.google.com/machine-learning/glossary#self-attention" rel="external nofollow noopener" target="_blank">self-attention</a> that applies the self-attention mechanism multiple times for each position in the input sequence.</p> <p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXddbpGsumHN_8HzHQWzr7aGJ2-_pDrAwnWplF2UprTtkShaYU73l6THa6NnIJ5hDRaPPZGjU2-i3P0ivylunKUEqexbWnEm_c5JRrpl21jPEUMpQPNPt12Wqb3XkeMZLZtSLnPyT-Fxji4v-yJ90w0E54Kh?key=f7C-3cR0JMOdp_HXQpeORQ" alt=""></p> <p>The structure of transformers looks like:</p> <p>Transformers (e.g. GPT-4, LaMDA, Wu-Dao) are neural networks that learn context and meaning by closely tracking relationships in sequential data. LLMs are <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="external nofollow noopener" target="_blank">transformer-based models</a>. </p> <p><strong>Transformer architecture</strong></p> <p>\</p> <p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXd3LRh8UajU1xRAIICPPBouHT5B-UXdRLX6C-qqXAHXOM7-eVSu-kGl-QhPQGoqiojaP3wciYXgcOAiM-AxX96iWUgihP6PWQO_rBLempOuhRf-d07EoYhIoJFL1_VcLk-v88CJVcImoLz3DZOhw9CV-TlL?key=f7C-3cR0JMOdp_HXQpeORQ" alt=""></p> <p>\</p> <p>Different types of architectures are relevant for different types of tasks: </p> <ul> <li> <p><strong>Encoder-only</strong>: Encoder only models use only the encoder of the transformer. It maps input text into an intermediate representation, aka embedding, which could serve as the input for downstream applications, like classifications. e.g. BERT. The one-model initiative in AnA is an example of an Encoder-only model.</p> </li> <li> <p><strong>Encoder-decoder</strong>: These are essentially seq-to-seq models. It adds a decoder which maps intermediate representations into text output, which are used for generating text with a given input, such as summarization, or translation. Attention allows the decoder network to focus on relevant parts of the input when producing an output i.e. enhances some parts of the input data while diminishing the others. The idea is that there might be relevant information in every word in a sentence. So in order for the decoding to be precise, it needs to take into account all words of the input, using attention. Adds an extra input to each decoding step that comes from the encoding steps. e.g. MUM</p> </li> <li> <p><strong>Decoder-only:</strong> Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. It is fed the original text, produces some output and this output is used as an input back into itself and these models are often called <strong>auto-regressive</strong> models. These models are well suited for tasks involving text generation. E.g. PaLM, ChatGPT</p> </li> </ul> <p>The structure of transformers looks like:</p> <p><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXe7NncVvfQIOJzJ_1xBbmiqKGYCxrf22x-677KIEFwNEq1DmrtcdprUSNdI7_DqwWm2mBgsfWzykuMKWgyIXclEs8HdWUO5JL-CQtGI2cWU5i_weklv3gF9yyobVYUHJtRS6xWLCIsuh2cFRyqUHEv2Y_52?key=f7C-3cR0JMOdp_HXQpeORQ" alt=""></p> <ul> <li>Attention architecture is mostly unchanged today, but some details have changed (positional embeddings, learning rates, pre-norm attention)</li> </ul> </div> </article> <h2>References</h2> <div class="publications"> <ol class="bibliography"></ol> </div> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/ModernBERT/">ModernBERT</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Positional-Embeddings/">Positional Embeddings</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/BERT/">BERT</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Arpita Saha. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>